# -*- coding: utf-8 -*-
"""similartiy scibert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lh1Q6ffAq9w1ysJQTxi0tfpWAFAje6Hj
"""

!pip install transformers

from transformers import AutoTokenizer, AutoModel
import torch

# Load SciBERT tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("allenai/scibert_scivocab_uncased")
model = AutoModel.from_pretrained("allenai/scibert_scivocab_uncased")

def sentence_similarity(sentence1, sentence2):
    # Tokenize the input sentences
    tokens = tokenizer.encode_plus(sentence1, sentence2, padding=True, truncation=True, return_tensors="pt")

    # Pass the tokenized input through the SciBERT model
    with torch.no_grad():
        output = model(**tokens)

    # Extract the sentence embeddings from the model's output
    sentence_embeddings = output.last_hidden_state[:, 0, :]

    # Calculate the cosine similarity between the sentence embeddings
    similarity = torch.cosine_similarity(sentence_embeddings[0], sentence_embeddings[1], dim=0)

    return similarity.item()

# Example usage
sentence1 = "The cat is sitting on the mat."
sentence2 = "The cat is lying on the mat."
similarity_score = sentence_similarity(sentence1, sentence2)
print("Similarity score:", similarity_score)

def sentence_similarity(sentence1, sentence2):
    # Tokenize the input sentences
    tokens = tokenizer.encode_plus(sentence1, sentence2, padding=True, truncation=True, return_tensors="pt")

    # Pass the tokenized input through the SciBERT model
    with torch.no_grad():
        output = model(**tokens)

    # Extract the sentence embeddings from the model's output
    sentence_embeddings = output.last_hidden_state[:, 0, :]

    # Calculate the cosine similarity between the sentence embeddings
    similarity = torch.cosine_similarity(sentence_embeddings, sentence_embeddings, dim=1)

    return similarity.item()

# Example usage
sentence1 = "The cat is sitting on the mat."
sentence2 = "The cat is lying on the mat."
similarity_score = sentence_similarity(sentence1, sentence2)
print("Similarity score:", similarity_score)









from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')

def get_sentence_embedding(sentence):
    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)
    outputs = model(**inputs)
    # Taking the mean of the last hidden state to get sentence embedding
    sentence_embedding = torch.mean(outputs.last_hidden_state[0], dim=0)
    return sentence_embedding

def cosine_similarity(a, b):
    return torch.nn.functional.cosine_similarity(a, b, dim=0)

def are_sentences_related(sentence1, sentence2, threshold=0.8):
    embedding1 = get_sentence_embedding(sentence1)
    embedding2 = get_sentence_embedding(sentence2)
    similarity = cosine_similarity(embedding1, embedding2)
    return similarity > threshold

# Test the function
print(are_sentences_related("The cat sits on the mat.", "A cat is sitting on a mat."))
print(are_sentences_related("The cat sits on the mat.", "I like to play basketball."))

sentence1 = "nlp is a conference topic"
sentence2 = "natural language processing is a subject"
are_sentences_related(sentence1, sentence2)

sentence1 = "Barack Obama was the 44th President of the United States."
sentence2 = "Michelle Obama is a lawyer and writer."
are_sentences_related(sentence1, sentence2)

sentence1 = "The study analyzed the effects of climate change on biodiversity."
sentence2 = "The research examined how global warming impacts species diversity."
are_sentences_related(sentence1, sentence2)



are_sentences_related(sentence1, sentence2)





from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')
model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')

def get_sentence_embedding_scibert(sentence):
    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)
    outputs = model(**inputs)
    # Taking the mean of the last hidden state to get sentence embedding
    sentence_embedding = torch.mean(outputs.last_hidden_state[0], dim=0)
    return sentence_embedding

def cosine_similarity(a, b):
    return torch.nn.functional.cosine_similarity(a, b, dim=0)

def are_sentences_related_scibert(sentence1, sentence2, threshold=0.8):
    embedding1 = get_sentence_embedding(sentence1)
    embedding2 = get_sentence_embedding(sentence2)
    similarity = cosine_similarity(embedding1, embedding2)
    return similarity > threshold

sentence1 = "The study analyzed the effects of climate change on biodiversity."
sentence2 = "The research examined how global warming impacts species diversity."
are_sentences_related_scibert(sentence1, sentence2)

sentence1 = "The process of photosynthesis in plants involves the conversion of carbon dioxide and water into glucose and oxygen, powered by sunlight."
sentence2 = "Cellular respiration in animals takes glucose and oxygen to produce carbon dioxide, water, and energy."
are_sentences_related_scibert(sentence1, sentence2)

print(are_sentences_related_scibert("The cat sits on the mat.", "I like to play  basketball"))

sentence1 = "Water is a compound made up of two hydrogen atoms bonded to one oxygen atom."
sentence2 = "H2O, also known as water, consists of two atoms of hydrogen and one atom of oxygen."
are_sentences_related_scibert(sentence1, sentence2)

sentence1 = "Water is a compound made up of two hydrogen atoms bonded to one oxygen atom."
sentence2 = "H2O, also known as water, consists of two atoms of hydrogen and one atom of oxygen."
are_sentences_related(sentence1, sentence2)

sentence1 = "Light behaves like a wave, exhibiting properties such as interference and diffraction."
sentence2 = "In some scenarios, light behaves like a particle, displaying characteristics of quantized energy levels, as in the photoelectric effect."


print('bert:', are_sentences_related(sentence1, sentence2))
print('scibert:', are_sentences_related_scibert(sentence1, sentence2))

sentence1 = "Newton's second law of motion states that the force acting on an object is equal to the mass of the object times its acceleration."
sentence2 = "In some scenarios, light behaves like a particle, displaying characteristics of quantized energy levels, as in the photoelectric effect."

print('bert:', are_sentences_related(sentence1, sentence2))
print('scibert:', are_sentences_related_scibert(sentence1, sentence2))

sentence1 = "DNA replication is the process where a double-stranded DNA molecule is copied to produce two identical DNA molecules."
sentence2 = "These two sentences may seem different, but they both refer to the principle of Newton's second law."

print('bert:', are_sentences_related(sentence1, sentence2))
print('scibert:', are_sentences_related_scibert(sentence1, sentence2))





import pandas as pd

sdp_act_data = pd.read_csv('/content/drive/MyDrive/Jun 2023/sdp_act/train.txt', sep="\t")

sdp_act_data



sdp_act_data['citation_context'].iloc[1]

sdp_act_data['cite_context_paragraph'].iloc[1]

list1 = eval(sdp_act_data['cite_context_paragraph'].iloc[1])
list1

result = []

for i in eval(sdp_act_data['cite_context_paragraph'].iloc[0]):
  if are_sentences_related_scibert(sdp_act_data['citation_context'].iloc[0], i):
    result.append(i)

result

eval(sdp_act_data['cite_context_paragraph'].iloc[0])













result = []

for i in eval(sdp_act_data['cite_context_paragraph'].iloc[5]):
  if are_sentences_related_scibert(sdp_act_data['citation_context'].iloc[5], i):
    result.append(i)

result

len(result)

eval(sdp_act_data['cite_context_paragraph'].iloc[5])

len(eval(sdp_act_data['cite_context_paragraph'].iloc[5]))

result = []

for i in eval(sdp_act_data['cite_context_paragraph'].iloc[6]):
  if are_sentences_related_scibert(sdp_act_data['citation_context'].iloc[6], i):
    result.append(i)

result

eval(sdp_act_data['cite_context_paragraph'].iloc[6])



result = []

for i in eval(sdp_act_data['cite_context_paragraph'].iloc[7]):
  if are_sentences_related_scibert(sdp_act_data['citation_context'].iloc[7], i):
    result.append(i)

result

eval(sdp_act_data['cite_context_paragraph'].iloc[7])



sdp_act_10 = sdp_act_data.iloc[:10]
sdp_act_10

type(sdp_act_10.loc[1,'cite_context_paragraph'])

import ast

# Function to convert string cell to a list
def convert_string_to_list(cell):
    try:
        return ast.literal_eval(cell)
    except (SyntaxError, ValueError):
        # Return the original value if it's not a valid string representation of a list
        return cell

sdp_act_10['cite_context_paragraph'] = sdp_act_10['cite_context_paragraph'].apply(convert_string_to_list)

sdp_act_10

type(sdp_act_10.loc[1,'cite_context_paragraph'])

sdp_act_10.loc[1,'cite_context_paragraph']



import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')

def get_sentence_embedding(sentence):
    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)
    outputs = model(**inputs)
    sentence_embedding = torch.mean(outputs.last_hidden_state[0], dim=0)
    return sentence_embedding

def cosine_similarity(a, b):
    return torch.nn.functional.cosine_similarity(a, b, dim=0)

def get_similar_sentences_bert(sentence, sentences_list, threshold=0.8):
    similar_sentences = []
    sentence_embedding = get_sentence_embedding(sentence)

    for s in sentences_list:
        current_embedding = get_sentence_embedding(s)
        similarity = cosine_similarity(sentence_embedding, current_embedding)
        if similarity > threshold:
            similar_sentences.append(s)
    return similar_sentences

# Assume df is your DataFrame with 'sentence' and 'sentences_list' columns

from tqdm import tqdm

pbar = tqdm(total=len(sdp_act_10))

# Counter variable to track the number of cells processed
counter = 0


def get_similar_sentences_with_progress(row):
    global counter
    counter += 1

    # Perform the computation and update the progress bar
    result = get_similar_sentences_bert(row['citation_context'], row['cite_context_paragraph'])
    pbar.update()


    return result

sdp_act_10['similar_sentences_bert'] = sdp_act_10.apply(get_similar_sentences_with_progress, axis=1)
# Close the progress bar
pbar.close()

sdp_act_10['similar_sentences_bert']

def get_list_length(lst):
    return len(lst)

sdp_act_10['similar_sentences_bert_length'] = sdp_act_10['similar_sentences_bert'].apply(get_list_length)
sdp_act_10[['similar_sentences_bert_length','cite_context_paragraph_length']]













import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')
model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')

def get_sentence_embedding(sentence):
    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)
    outputs = model(**inputs)
    sentence_embedding = torch.mean(outputs.last_hidden_state[0], dim=0)
    return sentence_embedding

def cosine_similarity(a, b):
    return torch.nn.functional.cosine_similarity(a, b, dim=0)

def get_similar_sentences_scibert(sentence, sentences_list, threshold=0.85):
    similar_sentences = []
    sentence_embedding = get_sentence_embedding(sentence)

    for s in sentences_list:
        current_embedding = get_sentence_embedding(s)
        similarity = cosine_similarity(sentence_embedding, current_embedding)
        if similarity > threshold:
            similar_sentences.append(s)
    return similar_sentences

# Assume df is your DataFrame with 'sentence' and 'sentences_list' columns

from tqdm import tqdm

pbar = tqdm(total=len(sdp_act_10))

# Counter variable to track the number of cells processed
counter = 0


def get_similar_sentences_with_progress(row):
    global counter
    counter += 1

    # Perform the computation and update the progress bar
    result = get_similar_sentences_scibert(row['citation_context'], row['cite_context_paragraph'])
    pbar.update()


    return result

sdp_act_10['similar_sentences_scibert'] = sdp_act_10.apply(get_similar_sentences_with_progress, axis=1)
# Close the progress bar
pbar.close()



sdp_act_10['similar_sentences_scibert_length'] = sdp_act_10['similar_sentences_scibert'].apply(get_list_length)

sdp_act_10[['similar_sentences_bert_length', 'similar_sentences_scibert_length','cite_context_paragraph_length']]



sdp_act_10.columns.tolist()

sdp_act_10.loc[9,'citation_context']

sdp_act_10.loc[9,'similar_sentences_bert']

sdp_act_10.loc[9,'similar_sentences_scibert']

sdp_act_10.loc[9,'cite_context_paragraph']





